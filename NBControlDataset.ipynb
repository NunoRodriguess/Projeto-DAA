{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33caacbe-7d6b-4ec8-8f38-aa78cfc69a7c",
   "metadata": {},
   "source": [
    "# Notebook do Grupo23 para o dataset Controlo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411e941-4cae-490f-8d7d-206d55230eed",
   "metadata": {},
   "source": [
    "## Primeira análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb438f04-850a-4ece-9bd6-cb7537009bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2a600-802c-4595-b1eb-83c2d29189b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "control = pd.read_csv(\"sbsppdaa24/train_radiomics_hipocamp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c0197-8d04-4213-8385-7867c2f82f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "control.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7956fc-51ff-4349-8128-9b7a8e420005",
   "metadata": {},
   "outputs": [],
   "source": [
    "control.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57758a0d-c840-4a6a-8771-273f12dfc97c",
   "metadata": {},
   "source": [
    "## Drop das primeiras Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e6a48e-5b3e-421d-9b0f-8fa7a8030df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "control['Mask'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaab9bf-dff8-4038-86bb-cf18c3864836",
   "metadata": {},
   "outputs": [],
   "source": [
    "control['ID'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7992c7e-7172-4bcc-900e-e36d908d533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "control['Image'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8cdad-9727-44c4-b21c-b1f3b5fffbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "control.drop(columns=[\"Mask\",\"ID\",\"Image\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff6bda-ae9d-4e6f-97bf-cd4102c77a85",
   "metadata": {},
   "source": [
    "### Os três atributos são únicos para todas as entradas e não são numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7dc3b-0230-46a9-9442-e9a1c57a0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and drop columns where all values are the same\n",
    "columns_to_drop = [col for col in control.columns if control[col].nunique() == 1]\n",
    "\n",
    "# Drop the columns\n",
    "control.drop(columns=columns_to_drop, inplace=True)\n",
    "print(f\"Dropped {len(columns_to_drop)} constant columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0005ae40-0992-4b9a-b628-c6823acbc266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop columns where all values are unique and the column is non-numeric\n",
    "columns_to_drop = [col for col in control.columns if control[col].dtype == 'object' and control[col].nunique() == len(control)]\n",
    "columns_to_drop\n",
    "# Drop the columns\n",
    "control.drop(columns=columns_to_drop, inplace=True)\n",
    "print(f\"Dropped {len(columns_to_drop)} unique non-numerical columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c320d8-cf49-4d57-84fb-51cd493c13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for columns that contain the word \"Hash\" in their name\n",
    "hash_columns = [col for col in control.columns if 'Hash' in col]\n",
    "\n",
    "control.drop(columns=hash_columns, inplace=True)\n",
    "# Print the number of columns that contain \"Hash\"\n",
    "print(f\"Number of columns containing 'Hash': {len(hash_columns)}\")\n",
    "\n",
    "# Optionally, print the names of these columns\n",
    "print(\"Columns containing 'Hash':\", hash_columns)\n",
    "control.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef11c435-afa9-4af2-b329-3c1c128cfd0b",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d58adab-c538-4172-a472-eee5a73baf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(control.isnull(),yticklabels=False,cbar=False,cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b23fa1-8b44-4690-9687-7e4228aa7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "control.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36520d8a-195c-40c5-83ff-982330dde8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for columns with missing values\n",
    "missing_values = control.isnull().sum()\n",
    "\n",
    "# Filter to display only columns where the number of missing values is greater than 0\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# Print the result\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4aec3-e100-426b-b9a8-f92d042e4782",
   "metadata": {},
   "source": [
    "## Handling Categoric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61a7d4-e380-4194-b33a-d453cce7bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns (data type 'object' or 'category')\n",
    "categorical_columns = control.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Iterate over each categorical column and print its unique categories\n",
    "for col in categorical_columns:\n",
    "    unique_values = control[col].unique()\n",
    "    print(f\"Column '{col}' has the following categories: {unique_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b79adf-ad33-4dda-8a8d-31b70b606a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_count = control['Transition'].value_counts()\n",
    "sns.barplot(x=transition_count.index,y=transition_count.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503039c-d835-431f-9385-a28310a744aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target muito unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea450587-9a4d-4273-9bbb-9abd1d665130",
   "metadata": {},
   "source": [
    "## Handeling Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebd8a8-a311-407b-b743-81d7c1d9b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with datetime64 data type\n",
    "date_columns = control.select_dtypes(include=['datetime64']).columns\n",
    "\n",
    "# Print the columns with date data type\n",
    "print(f\"Columns with datetime data type: {date_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1533f5ff-f34b-40aa-b4f9-2e31710c5107",
   "metadata": {},
   "source": [
    "## Handeling highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2093bc-73ee-43ca-9ecb-a0fef17dfb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_target = pd.get_dummies(control['Transition'], prefix='category')\n",
    "# Concatenate the one-hot encoded target back into your dataset\n",
    "control_encoded = pd.concat([control, encoded_target], axis=1)\n",
    "# Drop the original target column if needed\n",
    "control_encoded.drop(columns=['Transition'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4cf02-2b46-443a-ac18-4fa07f1f6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(500, 500))\n",
    "control_corr = control_encoded.corr(method='pearson')\n",
    "sns.heatmap(control_corr, linecolor='black', linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f2b2a-4fd1-4ca0-99e8-a04b9fd423b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demasiadas features para retirar alguma conclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4bb78-6369-4fee-b7f7-2ed1b1262f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Initialize the variance threshold\n",
    "selector = VarianceThreshold(threshold=0.01)  # Adjust the threshold as needed\n",
    "\n",
    "# Fit and transform the dataset to remove low-variance features\n",
    "control_cleaned = selector.fit_transform(control_encoded)\n",
    "\n",
    "print(f\"Remaining columns after variance thresholding: {control_cleaned.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834529b-6a15-47b5-a8c2-1654ab57db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "X = control.drop(columns=['Transition'])\n",
    "y = control['Transition']\n",
    "\n",
    "k = 8 # Number of top features to select\n",
    "selector = SelectKBest(f_classif, k=k)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Get the boolean mask of selected features\n",
    "selected_features_mask = selector.get_support()\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = X.columns[selected_features_mask]\n",
    "\n",
    "# Create a new DataFrame with the selected features\n",
    "controlKbest = control[selected_features.tolist() + ['Transition']]\n",
    "\n",
    "print(f\"Reduced dataset has {controlKbest.shape[1]} columns after feature selection.\")\n",
    "print(\"Selected features DataFrame:\")\n",
    "controlKbest.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548daad7-a66d-462a-b64c-1a2230fed626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = control.drop(columns=['Transition'])  # Features\n",
    "y = control['Transition']  # Target variable\n",
    "\n",
    "# Fit a RandomForestClassifier to compute feature importances\n",
    "model = RandomForestClassifier(n_estimators=500)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances and drop low-importance features\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]  # Sort by importance\n",
    "\n",
    "# Select top N features, for example, the top 20 features\n",
    "top_n = 80\n",
    "top_indices = indices[:top_n]\n",
    "\n",
    "# Create a new DataFrame with the selected features\n",
    "X_new = X.iloc[:, top_indices]\n",
    "\n",
    "# If you want to include the target variable as well\n",
    "controlKbestRandomForest = control.iloc[:, top_indices.tolist() + [control.columns.get_loc('Transition')]]\n",
    "\n",
    "print(f\"Reduced dataset has {controlKbest.shape[1]} columns after Random Forest feature selection.\")\n",
    "print(\"Selected features DataFrame:\")\n",
    "controlKbestRandomForest.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea78bdf0-8b00-4b2e-9301-80253a61bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_target = pd.get_dummies(controlKbest['Transition'], prefix='category')\n",
    "controlKbest_encoded = pd.concat([controlKbest, encoded_target], axis=1)\n",
    "controlKbest_encoded.drop(columns=['Transition'],inplace=True)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (5,5))\n",
    "controlKbest_corr = controlKbest_encoded.corr( method = \"pearson\")\n",
    "sns.heatmap(controlKbest_corr, linecolor='black', linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428f863-917c-496d-815c-ad574b645ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_target = pd.get_dummies(controlKbestRandomForest['Transition'], prefix='category')\n",
    "controlKbest_encoded = pd.concat([controlKbestRandomForest, encoded_target], axis=1)\n",
    "controlKbest_encoded.drop(columns=['Transition'],inplace=True)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (50,50))\n",
    "controlKbest_corr = controlKbest_encoded.corr( method = \"pearson\")\n",
    "sns.heatmap(controlKbest_corr, linecolor='black', linewidths=0.5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee1e0f-b45b-4ad5-9cc4-d2f1875eca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover algumas colunas\n",
    "# Assuming X_new is your DataFrame with the top 80 features\n",
    "# Calculate the correlation matrix\n",
    "\n",
    "encoded_target = pd.get_dummies(controlKbestRandomForest['Transition'], prefix='category')\n",
    "controlKbest_encoded = pd.concat([controlKbestRandomForest, encoded_target], axis=1)\n",
    "controlKbest_encoded.drop(columns=['Transition'],inplace=True)\n",
    "\n",
    "correlation_matrix = controlKbest_encoded.corr().abs()  # Use absolute values for correlation\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than a threshold (e.g., 0.9)\n",
    "threshold = 0.9\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "# Drop the highly correlated features\n",
    "X_new_reduced = X_new.drop(columns=to_drop)\n",
    "\n",
    "# If you want to keep the target variable in the final dataset\n",
    "controlKbestRandomForestReduced = pd.concat([X_new_reduced, control['Transition']], axis=1)\n",
    "\n",
    "print(f\"Reduced dataset has {controlKbestRandomForestReduced.shape[1]} columns after removing highly correlated features.\")\n",
    "print(\"Reduced DataFrame:\")\n",
    "\n",
    "encoded_target = pd.get_dummies(controlKbestRandomForestReduced['Transition'], prefix='category')\n",
    "controlKbest_encoded = pd.concat([controlKbestRandomForestReduced, encoded_target], axis=1)\n",
    "controlKbest_encoded.drop(columns=['Transition'],inplace=True)\n",
    "ig = plt.figure(figsize = (25,25))\n",
    "controlKbest_corr = controlKbest_encoded.corr( method = \"pearson\")\n",
    "sns.heatmap(controlKbest_corr, linecolor='black', linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b1b5f-fc6a-44a3-a780-f28910376120",
   "metadata": {},
   "source": [
    "### Agora Vamos modelar! ( Sem esquecer preparação de dados claro )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9bacb-eb31-4b8e-a226-b2173546c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlKbestRandomForestReduced.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a03e6-f9a6-4510-8ded-5ceed13b3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Normalizar\n",
    "\n",
    "# Select only the float columns\n",
    "float_cols = controlKbestRandomForestReduced.select_dtypes(include=['float'])\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_float_cols = pd.DataFrame(scaler.fit_transform(float_cols), columns=float_cols.columns)\n",
    "\n",
    "# Replace the original float columns with the scaled columns\n",
    "controlKbestRandomForestReduced[float_cols.columns] = scaled_float_cols\n",
    "controlKbestRandomForestReduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05693d84-6e7a-4dfe-863e-4a5566d9b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = controlKbestRandomForestReduced.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ff4aa-21da-47af-84a4-7d8e7e310ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo com Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "X = df.drop(columns=['Transition'])  # Feature columns\n",
    "y = df['Transition']  # Target column\n",
    "\n",
    "# Step 3: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2025)\n",
    "\n",
    "# Step 4: Initialize Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=1000,      # Number of boosting stages (trees)\n",
    "    learning_rate=0.01,     # Step size shrinkage (lower values make training more robust but slower)\n",
    "    max_depth=20,           # Maximum depth of the individual regression estimators\n",
    "    random_state=2025        # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Step 5: Fit the model on the training data\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Predict on the test data\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Step 8: Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Step 9: Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)  # Use a blue color map for visualization\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3b474-2838-4af3-b2da-2d832b82bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo com Gradient Boosting + SMOTE\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Step 2: Prepare your data\n",
    "X = df.drop(columns=['Transition'])  # Feature columns\n",
    "y = df['Transition']  # Target column\n",
    "\n",
    "# Step 3: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2025)\n",
    "\n",
    "# Step 4: Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=2025)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Optional: Check the class distribution after SMOTE\n",
    "print(f\"Original class distribution: {Counter(y_train)}\")\n",
    "print(f\"Resampled class distribution: {Counter(y_train_resampled)}\")\n",
    "\n",
    "# Step 5: Initialize Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=1000,      # Number of boosting stages (trees)\n",
    "    learning_rate=0.01,     # Step size shrinkage (lower values make training more robust but slower)\n",
    "    max_depth=20,           # Maximum depth of the individual regression estimators\n",
    "    random_state=2025        # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Step 6: Fit the model on the resampled training data\n",
    "gb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Step 7: Predict on the original test data (not resampled)\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Step 9: Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Step 10: Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)  # Use a blue color map for visualization\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e36d07-0996-4427-b42b-0212a1040cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"sbsppdaa24/test_radiomics_hipocamp.csv\")\n",
    "\n",
    "# Apply the same preprocessing steps as we did for training data\n",
    "# 1. Drop unnecessary columns\n",
    "test_data.drop(columns=[\"Mask\", \"ID\", \"Image\"], inplace=True)\n",
    "\n",
    "# 2. Drop constant columns (use the same columns we dropped in training)\n",
    "columns_to_drop = [col for col in test_data.columns if test_data[col].nunique() == 1]\n",
    "test_data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# 3. Drop Hash columns\n",
    "hash_columns = [col for col in test_data.columns if 'Hash' in col]\n",
    "test_data.drop(columns=hash_columns, inplace=True)\n",
    "\n",
    "# 4. Keep only the features we used in training (from controlKbestRandomForestReduced)\n",
    "test_features = test_data[X.columns].copy()\n",
    "\n",
    "# 5. Apply the same scaling\n",
    "scaler = MinMaxScaler()\n",
    "float_cols = test_features.select_dtypes(include=['float'])\n",
    "scaled_float_cols = pd.DataFrame(\n",
    "    scaler.fit_transform(float_cols), \n",
    "    columns=float_cols.columns\n",
    ")\n",
    "\n",
    "# Use .loc to avoid the SettingWithCopyWarning\n",
    "test_features.loc[:, float_cols.columns] = scaled_float_cols\n",
    "\n",
    "# Make predictions using both models\n",
    "# Model without SMOTE\n",
    "predictions_no_smote = gb_model.predict(test_features)\n",
    "\n",
    "# Model with SMOTE\n",
    "predictions_with_smote = gb_model.predict(test_features)  # Using the SMOTE-trained model\n",
    "\n",
    "# Create DataFrames with predictions, starting from ID 1\n",
    "results_no_smote = pd.DataFrame({\n",
    "    'RowId': range(1, len(predictions_no_smote) + 1),\n",
    "    'Result': predictions_no_smote\n",
    "})\n",
    "\n",
    "results_with_smote = pd.DataFrame({\n",
    "    'RowId': range(1, len(predictions_with_smote) + 1),\n",
    "    'Result': predictions_with_smote\n",
    "})\n",
    "\n",
    "# Save predictions to CSV files\n",
    "results_no_smote.to_csv('predictions_no_smote.csv', index=False)\n",
    "results_with_smote.to_csv('predictions_with_smote.csv', index=False)\n",
    "\n",
    "# Print first few predictions from both models\n",
    "print(\"First few predictions without SMOTE:\")\n",
    "print(results_no_smote.head())\n",
    "print(\"\\nFirst few predictions with SMOTE:\")\n",
    "print(results_with_smote.head())\n",
    "\n",
    "# Calculate prediction distribution\n",
    "print(\"\\nPrediction distribution without SMOTE:\")\n",
    "print(pd.Series(predictions_no_smote).value_counts())\n",
    "print(\"\\nPrediction distribution with SMOTE:\")\n",
    "print(pd.Series(predictions_with_smote).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94fcf0-582b-42ae-af22-cf47d785bd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8912b138-d327-44fd-aa7c-2ddb9813495d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
